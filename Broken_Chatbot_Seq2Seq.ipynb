{
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "Broken Chatbot Seq2Seq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30763,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shabahmd/Machine-Learning-Notebooks/blob/main/Broken_Chatbot_Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d rajathmc/cornell-moviedialog-corpus  --unzip\n"
      ],
      "metadata": {
        "id": "Y-Ak0emuHPc1",
        "execution": {
          "iopub.status.busy": "2024-09-16T16:06:09.74194Z",
          "iopub.execute_input": "2024-09-16T16:06:09.742925Z",
          "iopub.status.idle": "2024-09-16T16:06:12.01785Z",
          "shell.execute_reply.started": "2024-09-16T16:06:09.742882Z",
          "shell.execute_reply": "2024-09-16T16:06:12.016704Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n"
      ],
      "metadata": {
        "id": "2f4LYud1IYrA",
        "execution": {
          "iopub.status.busy": "2024-09-16T16:05:56.315288Z",
          "iopub.execute_input": "2024-09-16T16:05:56.316338Z",
          "iopub.status.idle": "2024-09-16T16:06:00.419085Z",
          "shell.execute_reply.started": "2024-09-16T16:05:56.316291Z",
          "shell.execute_reply": "2024-09-16T16:06:00.41824Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('hello world')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-16T16:06:03.207041Z",
          "iopub.execute_input": "2024-09-16T16:06:03.207979Z",
          "iopub.status.idle": "2024-09-16T16:06:03.212861Z",
          "shell.execute_reply.started": "2024-09-16T16:06:03.207937Z",
          "shell.execute_reply": "2024-09-16T16:06:03.21187Z"
        },
        "trusted": true,
        "id": "hQelT3hBtBYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "with open('/kaggle/working/movie_lines.txt', encoding='utf-8', errors='replace') as f:\n",
        "    lines = f.readlines()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-16T16:06:29.30316Z",
          "iopub.execute_input": "2024-09-16T16:06:29.30389Z",
          "iopub.status.idle": "2024-09-16T16:06:29.3971Z",
          "shell.execute_reply.started": "2024-09-16T16:06:29.303841Z",
          "shell.execute_reply": "2024-09-16T16:06:29.396309Z"
        },
        "trusted": true,
        "id": "QabAQWmWtBYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-16T16:06:26.625155Z",
          "iopub.execute_input": "2024-09-16T16:06:26.625598Z",
          "iopub.status.idle": "2024-09-16T16:06:26.849846Z",
          "shell.execute_reply.started": "2024-09-16T16:06:26.625556Z",
          "shell.execute_reply": "2024-09-16T16:06:26.848928Z"
        },
        "trusted": true,
        "id": "bitesm1ytBYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filepath: chatbot_seq2seq_nltk.py\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Step 2: Load and preprocess the dataset\n",
        "def load_and_preprocess_data(file_path):\n",
        "    # Load the dataset\n",
        "    with open('/kaggle/working/movie_lines.txt', 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    # Extract conversations\n",
        "    conversations = []\n",
        "    for line in lines:\n",
        "        parts = line.split(\" +++$+++ \")\n",
        "        if len(parts) == 5:\n",
        "            conversations.append(parts[4].strip())\n",
        "\n",
        "    return conversations\n",
        "\n",
        "# Step 3: Clean the text using regex\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "# Step 4: Tokenize and pad sequences\n",
        "# Step 4: Tokenize and pad sequences, including <start> and <end> tokens\n",
        "def preprocess_and_tokenize(conversations, max_length):\n",
        "    # Clean and tokenize the conversations\n",
        "    conversations_cleaned = [clean_text(conv) for conv in conversations]\n",
        "\n",
        "    # Add <start> and <end> tokens to each sentence\n",
        "    questions = ['<start> ' + q + ' <end>' for q in conversations_cleaned]\n",
        "    answers = ['<start> ' + a + ' <end>' for a in conversations_cleaned]\n",
        "\n",
        "    tokenizer = Tokenizer(filters='', lower=True)\n",
        "    tokenizer.fit_on_texts(questions + answers)\n",
        "\n",
        "    sequences_questions = tokenizer.texts_to_sequences(questions)\n",
        "    sequences_answers = tokenizer.texts_to_sequences(answers)\n",
        "\n",
        "    # Pad sequences\n",
        "    questions_padded = pad_sequences(sequences_questions, maxlen=max_length, padding='post')\n",
        "    answers_padded = pad_sequences(sequences_answers, maxlen=max_length, padding='post')\n",
        "\n",
        "    return tokenizer, questions_padded, answers_padded, len(tokenizer.word_index) + 1\n",
        "\n",
        "\n",
        "# Load, preprocess, and tokenize data\n",
        "tokenizer, questions_padded, answers_padded, vocab_size = preprocess_and_tokenize(conversations, max_length=20)\n"
      ],
      "metadata": {
        "id": "3qjNhXyWIxns",
        "execution": {
          "iopub.status.busy": "2024-09-16T16:18:42.994072Z",
          "iopub.execute_input": "2024-09-16T16:18:42.994443Z",
          "iopub.status.idle": "2024-09-16T16:19:06.5797Z",
          "shell.execute_reply.started": "2024-09-16T16:18:42.99441Z",
          "shell.execute_reply": "2024-09-16T16:19:06.57863Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filepath: chatbot_seq2seq_nltk.py\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Step 1: Define the Encoder\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x)\n",
        "        return output, state\n",
        "\n",
        "# Step 2: Define the Attention mechanism\n",
        "\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(Attention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # query: shape (batch_size, hidden_size)\n",
        "        # values: shape (batch_size, max_len, hidden_size)\n",
        "\n",
        "        # If the query is missing a batch dimension, expand it\n",
        "        if len(query.shape) == 1:\n",
        "            query = tf.expand_dims(query, 0)  # Add batch dimension to query: (1, hidden_size)\n",
        "\n",
        "        # Expand query to have the time axis for broadcasting\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)  # (batch_size, 1, hidden_size)\n",
        "\n",
        "        # Calculate score\n",
        "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)  # (batch_size, max_len, 1)\n",
        "\n",
        "        # Multiply attention weights with the values to get the context vector\n",
        "        context_vector = attention_weights * values  # (batch_size, max_len, hidden_size)\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)  # (batch_size, hidden_size)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "# Step 3: Define the Decoder with Attention\n",
        "# Define the Decoder with the updated attention mechanism\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        self.attention = Attention(self.dec_units)\n",
        "\n",
        "    def call(self, x, enc_output, hidden):\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "        x = self.embedding(x)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        output, state = self.gru(x)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        x = self.fc(output)\n",
        "        return x, state, attention_weights\n",
        "\n",
        "\n",
        "# Instantiate the models\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "encoder = Encoder(vocab_size, embedding_dim, units)\n",
        "decoder = Decoder(vocab_size, embedding_dim, units)\n",
        "\n",
        "# Step 4: Define the loss function and optimizer\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Training step (simplified)\n",
        "def train_step(input_seq, target_seq, encoder, decoder, batch_size):\n",
        "    enc_output, enc_hidden = encoder(input_seq)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * batch_size, 1)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        for t in range(1, target_seq.shape[1]):\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, enc_output, dec_hidden)\n",
        "            loss = loss_function(target_seq[:, t], predictions)\n",
        "            dec_input = tf.expand_dims(target_seq[:, t], 1)\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "CBiUooLAMygN",
        "execution": {
          "iopub.status.busy": "2024-09-16T16:34:17.765374Z",
          "iopub.execute_input": "2024-09-16T16:34:17.765771Z",
          "iopub.status.idle": "2024-09-16T16:34:17.807979Z",
          "shell.execute_reply.started": "2024-09-16T16:34:17.765734Z",
          "shell.execute_reply": "2024-09-16T16:34:17.807251Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "updated attention mechaninsg\n"
      ],
      "metadata": {
        "id": "YMsp8x1ftBYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference: Generating responses\n",
        "def generate_response(input_sentence):\n",
        "    input_seq = tokenizer.texts_to_sequences([clean_text(input_sentence)])\n",
        "    input_seq = pad_sequences(input_seq, maxlen=20, padding='post')\n",
        "\n",
        "    enc_output, enc_hidden = encoder(input_seq)\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    # Start the sequence with <start> token\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "    result = ''\n",
        "    for t in range(20):\n",
        "        predictions, dec_hidden, _ = decoder(dec_input, enc_output, dec_hidden)\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        # If we predict <end>, stop the generation\n",
        "        if tokenizer.index_word[predicted_id] == '<end>':\n",
        "            break\n",
        "\n",
        "        # Add the predicted word to the result\n",
        "        result += tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "        # Use the predicted word as the next decoder input\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result.strip()\n",
        "\n",
        "# Example interaction\n",
        "response = generate_response(\"You are broken!\")\n",
        "print(\"Chatbot response:\", response)\n"
      ],
      "metadata": {
        "id": "C-CaFAN-NfmP",
        "execution": {
          "iopub.status.busy": "2024-09-16T16:34:20.675785Z",
          "iopub.execute_input": "2024-09-16T16:34:20.676565Z",
          "iopub.status.idle": "2024-09-16T16:34:21.661854Z",
          "shell.execute_reply.started": "2024-09-16T16:34:20.676524Z",
          "shell.execute_reply": "2024-09-16T16:34:21.660775Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = generate_response(\"how are you?\")\n",
        "\n",
        "print(\"Chatbot response:\", response)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-16T16:34:42.944758Z",
          "iopub.execute_input": "2024-09-16T16:34:42.945177Z",
          "iopub.status.idle": "2024-09-16T16:34:43.45926Z",
          "shell.execute_reply.started": "2024-09-16T16:34:42.945135Z",
          "shell.execute_reply": "2024-09-16T16:34:43.458338Z"
        },
        "trusted": true,
        "id": "DSRJYeQDtBYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Function to evaluate the chatbot responses using BLEU score\n",
        "def evaluate_bleu(reference, hypothesis):\n",
        "    reference_tokens = [reference.split()]  # Tokenize the reference (ground truth)\n",
        "    hypothesis_tokens = hypothesis.split()   # Tokenize the generated response\n",
        "\n",
        "    # Apply BLEU scoring\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    score = sentence_bleu(reference_tokens, hypothesis_tokens, smoothing_function=smoothie)\n",
        "    return score\n",
        "\n",
        "# Example interaction and evaluation\n",
        "user_input = \"Hello!\"\n",
        "generated_response = generate_response(user_input)\n",
        "reference_response = \"Hi, how can I help you?\"  # Ground truth response\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_score = evaluate_bleu(reference_response, generated_response)\n",
        "print(f\"Generated Response: {generated_response}\")\n",
        "print(f\"Reference Response: {reference_response}\")\n",
        "print(f\"BLEU Score: {bleu_score}\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-16T16:35:31.050709Z",
          "iopub.execute_input": "2024-09-16T16:35:31.051437Z",
          "iopub.status.idle": "2024-09-16T16:35:31.537635Z",
          "shell.execute_reply.started": "2024-09-16T16:35:31.051393Z",
          "shell.execute_reply": "2024-09-16T16:35:31.536633Z"
        },
        "trusted": true,
        "id": "SZCQeePLtBYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rouge-score\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-16T16:35:52.312906Z",
          "iopub.execute_input": "2024-09-16T16:35:52.313653Z",
          "iopub.status.idle": "2024-09-16T16:36:08.383038Z",
          "shell.execute_reply.started": "2024-09-16T16:35:52.313615Z",
          "shell.execute_reply": "2024-09-16T16:36:08.381863Z"
        },
        "trusted": true,
        "id": "bNotN7TntBYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Function to evaluate chatbot responses using ROUGE score\n",
        "def evaluate_rouge(reference, hypothesis):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score(reference, hypothesis)\n",
        "    return scores\n",
        "\n",
        "# Example interaction and evaluation\n",
        "user_input = \"Hello!\"\n",
        "generated_response = generate_response(user_input)\n",
        "reference_response = \"Hi, how can I help you?\"  # Ground truth response\n",
        "\n",
        "# Calculate ROUGE score\n",
        "rouge_scores = evaluate_rouge(reference_response, generated_response)\n",
        "print(f\"Generated Response: {generated_response}\")\n",
        "print(f\"Reference Response: {reference_response}\")\n",
        "print(f\"ROUGE-1 Score: {rouge_scores['rouge1'].fmeasure}\")\n",
        "print(f\"ROUGE-L Score: {rouge_scores['rougeL'].fmeasure}\")\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-16T16:36:08.385015Z",
          "iopub.execute_input": "2024-09-16T16:36:08.385373Z",
          "iopub.status.idle": "2024-09-16T16:36:08.890579Z",
          "shell.execute_reply.started": "2024-09-16T16:36:08.385338Z",
          "shell.execute_reply": "2024-09-16T16:36:08.889693Z"
        },
        "trusted": true,
        "id": "hrUFfV9KtBYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def perplexity(model, X, y):\n",
        "    # X is the input sequence, y is the target sequence\n",
        "    preds = model.predict(X)  # Predictions from the model\n",
        "    cross_entropy = -np.mean(np.log(np.max(preds, axis=1)))  # Compute cross-entropy\n",
        "    return np.exp(cross_entropy)  # Return perplexity\n",
        "\n",
        "# Assuming you have X_test and y_test ready\n",
        "perplexity_score = perplexity(encoder_decoder_model, X_test, y_test)\n",
        "print(f\"Perplexity Score: {perplexity_score}\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-16T16:36:52.372933Z",
          "iopub.execute_input": "2024-09-16T16:36:52.373338Z",
          "iopub.status.idle": "2024-09-16T16:36:52.427245Z",
          "shell.execute_reply.started": "2024-09-16T16:36:52.373301Z",
          "shell.execute_reply": "2024-09-16T16:36:52.426113Z"
        },
        "trusted": true,
        "id": "WKgeNn_htBYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_accuracy(true_responses, generated_responses):\n",
        "    correct = 0\n",
        "    for true, generated in zip(true_responses, generated_responses):\n",
        "        if true == generated:\n",
        "            correct += 1\n",
        "    return correct / len(true_responses)\n",
        "\n",
        "# Example usage:\n",
        "true_responses = [\"Hi, how can I help you?\", \"Goodbye!\", \"What's your name?\"]\n",
        "generated_responses = [generate_response(\"Hello!\"), generate_response(\"Bye!\"), generate_response(\"What's your name?\")]\n",
        "\n",
        "accuracy = custom_accuracy(true_responses, generated_responses)\n",
        "print(f\"Custom Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-16T16:37:11.657547Z",
          "iopub.execute_input": "2024-09-16T16:37:11.658658Z",
          "iopub.status.idle": "2024-09-16T16:37:13.025894Z",
          "shell.execute_reply.started": "2024-09-16T16:37:11.6586Z",
          "shell.execute_reply": "2024-09-16T16:37:13.024826Z"
        },
        "trusted": true,
        "id": "iC-0SYi2tBYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YYuzijJFtBYe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}